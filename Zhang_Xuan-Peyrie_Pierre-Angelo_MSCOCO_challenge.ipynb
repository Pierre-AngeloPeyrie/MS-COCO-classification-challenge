{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "98da5ac3",
      "metadata": {},
      "source": [
        "# The MS COCO classification challenge\n",
        "\n",
        "Razmig Kéchichian\n",
        "\n",
        "This notebook defines the multi-class classification challenge on the [MS COCO dataset](https://cocodataset.org/). It defines the problem, sets the rules of organization and presents tools you are provided with to accomplish the challenge.\n",
        "\n",
        "\n",
        "## 1. Problem statement\n",
        "\n",
        "Each image has **several** categories of objects to predict, hence the difference compared to the classification problem we have seen on the CIFAR10 dataset where each image belonged to a **single** category, therefore the network loss function and prediction mechanism (only highest output probability) were defined taking this constraint into account.\n",
        "\n",
        "We adapted the MS COCO dataset for the requirements of this challenge by, among other things, reducing the number of images and their dimensions to facilitate processing.\n",
        "\n",
        "In the companion `ms-coco.zip` compressed directory you will find two sub-directories:\n",
        "- `images`: which contains the images in train (65k) and test (~5k) subsets,\n",
        "- `labels`: which lists labels for each of the images in the train subset only.\n",
        "\n",
        "Each label file gives a list of class IDs that correspond to the class index in the following tuple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2acb621b",
      "metadata": {},
      "outputs": [],
      "source": [
        "classes = (\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \n",
        "           \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
        "           \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",       \n",
        "           \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
        "           \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
        "           \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \n",
        "           \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \n",
        "           \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \n",
        "           \"hair drier\", \"toothbrush\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bf52f93",
      "metadata": {},
      "source": [
        "Your goal is to follow a **transfer learning strategy** in training and validating a network on **your own distribution of training data into training and a validation subsets**, then to **test it on the test subset** by producing a [JSON file](https://en.wikipedia.org/wiki/JSON) with content of the following format:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"000000000139\": [\n",
        "        56,\n",
        "        60,\n",
        "        62\n",
        "    ],\n",
        "    \"000000000285\": [\n",
        "        21,\n",
        "    ],\n",
        "    \"000000000632\": [\n",
        "        57,\n",
        "        59,\n",
        "    73\n",
        "    ],\n",
        "    # other test images\n",
        "}\n",
        "```\n",
        "\n",
        "In this file, the name (without extension) of each test image is associated with a list of class indices predicted by your network. Make sure that the JSON file you produce **follows this format strictly**.\n",
        "\n",
        "You will submit your JSON prediction file to the following [online evaluation server and leaderboard](https://www.creatis.insa-lyon.fr/kechichian/ms-coco-classif-leaderboard.html), which will evaluate your predictions on test set labels, unavailable to you.\n",
        "\n",
        "<div class=\"alert alert-block alert-danger\"> <b>WARNING:</b> Use this server with <b>the greatest care</b>. A new submission with identical Participant or group name will <b>overwrite</b> the identically named submission, if one already exists, therefore check the leaderboard first. <b>Do not make duplicate leaderboard entries for your group</b>, keep track of your test scores privately. Also pay attention to upload only JSON files of the required format.<br>\n",
        "</div>\n",
        "\n",
        "The evaluation server calculates and returns mean performances over all classes, and optionally per class performances. Entries in the leaderboard are sorted by the F1 metric.\n",
        "\n",
        "You can request an evaluation as many times as you want. It is up to you to specify the final evaluation by updating the leaderboard entry corresponding to your Participant or group name. This entry will be taken into account for grading your work.\n",
        "\n",
        "It goes without saying that it is **prohibited** to use another distribution of the MS COCO database for training, e.g. the Torchvision dataset.\n",
        "\n",
        "\n",
        "## 2. Instructions\n",
        "This notebook needs to be in the root directory of the environement with the dataseet directory **ms-coco**. The dependancies are listed in the **requirements.txt** file which comes with this notebook and the python version used to run this notebook is **ver=3.13.0** it may work with anterior versions but there is no guarantee. The cells have to be executed sequentialy. The `label_stats.npy` is included to skip th calculation of these stats. It must be in the root directory of the environement.\n",
        "    \n",
        "    \n",
        "## 3. Tools\n",
        "\n",
        "### 3.1 Custom `Dataset`s with data augmentation\n",
        "\n",
        "We provide you with two custom `torch.utils.data.Dataset` sub-classes to use in training and testing. The `COCOTrainImageDataset` was adapted to support other directories containing transformed images for data augmantation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34dd4b64",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "\n",
        "class COCOTrainImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir, annotations_dir, max_images=None, transform=None,data_augmentation_paths = None):\n",
        "        self.img_labels = sorted(glob(\"*.cls\", root_dir=annotations_dir))\n",
        "        self.data_augmentation_paths = data_augmentation_paths\n",
        "        if self.data_augmentation_paths is not None :\n",
        "            self.img_labels.extend(sorted(glob(\"*.cls\", root_dir=self.data_augmentation_paths[\"labels\"])))\n",
        "        if max_images:\n",
        "            self.img_labels = self.img_labels[:max_images]\n",
        "        self.img_dir = img_dir\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.data_augmentation_paths[\"prefix\"] in self.img_labels[idx]:\n",
        "            img_path = os.path.join(self.data_augmentation_paths[\"images\"], Path(self.img_labels[idx]).stem + \".jpg\")\n",
        "            labels_path = os.path.join(self.data_augmentation_paths[\"labels\"], self.img_labels[idx])\n",
        "        else:\n",
        "            img_path = os.path.join(self.img_dir, Path(self.img_labels[idx]).stem + \".jpg\")\n",
        "            labels_path = os.path.join(self.annotations_dir, self.img_labels[idx])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        with open(labels_path) as f: \n",
        "            labels = [int(label) for label in f.readlines()]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        labels = torch.zeros(80).scatter_(0, torch.tensor(labels), value=1)\n",
        "        \n",
        "        return image, labels\n",
        "\n",
        "\n",
        "class COCOTestImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        self.img_list = sorted(glob(\"*.jpg\", root_dir=img_dir))    \n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_list[idx])\n",
        "        image = Image.open(img_path).convert(\"RGB\")        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, Path(img_path).stem # filename w/o extension\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7d805e2",
      "metadata": {},
      "source": [
        "### 3.2 Training and validation loops\n",
        "\n",
        "The following are two general-purpose classification train and validation loop functions to be called inside the epochs for-loop with appropriate argument settings.\n",
        "\n",
        "The training loop was changed to monitor the `running_loss` for each epoch. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb693462",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def train_loop(train_loader, net, criterion, optimizer, device,\n",
        "               mbatch_loss_group=-1):\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    mbatch_losses = []\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()*inputs.size(0)\n",
        "        # following condition False by default, unless mbatch_loss_group > 0\n",
        "        if i % mbatch_loss_group == mbatch_loss_group - 1:\n",
        "            mbatch_losses.append(running_loss / mbatch_loss_group)\n",
        "            running_loss = 0.0\n",
        "            \n",
        "    results = running_loss/len(train_loader.dataset)\n",
        "    if mbatch_loss_group > 0:\n",
        "        results = results, mbatch_losses\n",
        "    return results\n",
        "\n",
        "\n",
        "def validation_loop(val_loader, net, criterion, num_classes, device,\n",
        "                    multi_task=False, th_multi_task=0.5, one_hot=False, class_metrics=False):\n",
        "    net.eval()\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    size = len(val_loader.dataset)\n",
        "    class_total = {label:0 for label in range(num_classes)}\n",
        "    class_tp = {label:0 for label in range(num_classes)}\n",
        "    class_fp = {label:0 for label in range(num_classes)}\n",
        "    with torch.no_grad():\n",
        "        for data in val_loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item() * images.size(0)\n",
        "            if not multi_task:    \n",
        "                predictions = torch.zeros_like(outputs)\n",
        "                predictions[torch.arange(outputs.shape[0]), torch.argmax(outputs, dim=1)] = 1.0\n",
        "            else:\n",
        "                predictions = torch.where(outputs > th_multi_task, 1.0, 0.0)\n",
        "            if not one_hot:\n",
        "                labels_mat = torch.zeros_like(outputs)\n",
        "                labels_mat[torch.arange(outputs.shape[0]), labels] = 1.0\n",
        "                labels = labels_mat\n",
        "                \n",
        "            tps = predictions * labels\n",
        "            fps = predictions - tps\n",
        "            \n",
        "            tps = tps.sum(dim=0)\n",
        "            fps = fps.sum(dim=0)\n",
        "            lbls = labels.sum(dim=0)  \n",
        "                \n",
        "            for c in range(num_classes):\n",
        "                class_tp[c] += tps[c]\n",
        "                class_fp[c] += fps[c]\n",
        "                class_total[c] += lbls[c]\n",
        "                    \n",
        "            correct += tps.sum()\n",
        "\n",
        "    class_prec = []\n",
        "    class_recall = []\n",
        "    freqs = []\n",
        "    for c in range(num_classes):\n",
        "        class_prec.append(0 if class_tp[c] == 0 else\n",
        "                          class_tp[c] / (class_tp[c] + class_fp[c]))\n",
        "        class_recall.append(0 if class_tp[c] == 0 else\n",
        "                            class_tp[c] / class_total[c])\n",
        "        freqs.append(class_total[c])\n",
        "\n",
        "    freqs = torch.tensor(freqs)\n",
        "    class_weights = 1. / freqs\n",
        "    class_weights /= class_weights.sum()\n",
        "    class_prec = torch.tensor(class_prec)\n",
        "    class_recall = torch.tensor(class_recall)\n",
        "    prec = (class_prec * class_weights).sum()\n",
        "    recall = (class_recall * class_weights).sum()\n",
        "    f1 = 2. / (1/prec + 1/recall)\n",
        "    val_loss = loss / size\n",
        "    accuracy = correct / freqs.sum()\n",
        "    results = {\"loss\": val_loss, \"accuracy\": accuracy, \"f1\": f1,\\\n",
        "               \"precision\": prec, \"recall\": recall}\n",
        "\n",
        "    if class_metrics:\n",
        "        class_results = []\n",
        "        for p, r in zip(class_prec, class_recall):\n",
        "            f1 = (0 if p == r == 0 else 2. / (1/p + 1/r))\n",
        "            class_results.append({\"f1\": f1, \"precision\": p, \"recall\": r})\n",
        "        results = results, class_results\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "311d8e84",
      "metadata": {},
      "source": [
        "### 3.3 Tensorboard logging (optional)\n",
        "\n",
        "Evaluation metrics and losses produced by the `validation_loop()` function on train and validation data can be logged to a [Tensorboard `SummaryWriter`](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) which allows you to observe training graphically via the following function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "020a27cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_graphs(summary_writer, epoch, train_results, test_results,\n",
        "                  train_class_results=None, test_class_results=None, \n",
        "                  class_names = None, mbatch_group=-1, mbatch_count=0, mbatch_losses=None):\n",
        "    if mbatch_group > 0:\n",
        "        for i in range(len(mbatch_losses)):\n",
        "            summary_writer.add_scalar(\"Losses/Train mini-batches\",\n",
        "                                  mbatch_losses[i],\n",
        "                                  epoch * mbatch_count + (i+1)*mbatch_group)\n",
        "\n",
        "    summary_writer.add_scalars(\"Losses/Train Loss vs Test Loss\",\n",
        "                               {\"Train Loss\" : train_results[\"loss\"],\n",
        "                                \"Test Loss\" : test_results[\"loss\"]},\n",
        "                               (epoch + 1) if not mbatch_group > 0\n",
        "                                     else (epoch + 1) * mbatch_count)\n",
        "\n",
        "    summary_writer.add_scalars(\"Metrics/Train Accuracy vs Test Accuracy\",\n",
        "                               {\"Train Accuracy\" : train_results[\"accuracy\"],\n",
        "                                \"Test Accuracy\" : test_results[\"accuracy\"]},\n",
        "                               (epoch + 1) if not mbatch_group > 0\n",
        "                                     else (epoch + 1) * mbatch_count)\n",
        "\n",
        "    summary_writer.add_scalars(\"Metrics/Train F1 vs Test F1\",\n",
        "                               {\"Train F1\" : train_results[\"f1\"],\n",
        "                                \"Test F1\" : test_results[\"f1\"]},\n",
        "                               (epoch + 1) if not mbatch_group > 0\n",
        "                                     else (epoch + 1) * mbatch_count)\n",
        "\n",
        "    summary_writer.add_scalars(\"Metrics/Train Precision vs Test Precision\",\n",
        "                               {\"Train Precision\" : train_results[\"precision\"],\n",
        "                                \"Test Precision\" : test_results[\"precision\"]},\n",
        "                               (epoch + 1) if not mbatch_group > 0\n",
        "                                     else (epoch + 1) * mbatch_count)\n",
        "\n",
        "    summary_writer.add_scalars(\"Metrics/Train Recall vs Test Recall\",\n",
        "                               {\"Train Recall\" : train_results[\"recall\"],\n",
        "                                \"Test Recall\" : test_results[\"recall\"]},\n",
        "                               (epoch + 1) if not mbatch_group > 0\n",
        "                                     else (epoch + 1) * mbatch_count)\n",
        "\n",
        "    if train_class_results and test_class_results:\n",
        "        for i in range(len(train_class_results)):\n",
        "            summary_writer.add_scalars(f\"Class Metrics/{class_names[i]}/Train F1 vs Test F1\",\n",
        "                                       {\"Train F1\" : train_class_results[i][\"f1\"],\n",
        "                                        \"Test F1\" : test_class_results[i][\"f1\"]},\n",
        "                                       (epoch + 1) if not mbatch_group > 0\n",
        "                                             else (epoch + 1) * mbatch_count)\n",
        "\n",
        "            summary_writer.add_scalars(f\"Class Metrics/{class_names[i]}/Train Precision vs Test Precision\",\n",
        "                                       {\"Train Precision\" : train_class_results[i][\"precision\"],\n",
        "                                        \"Test Precision\" : test_class_results[i][\"precision\"]},\n",
        "                                       (epoch + 1) if not mbatch_group > 0\n",
        "                                             else (epoch + 1) * mbatch_count)\n",
        "\n",
        "            summary_writer.add_scalars(f\"Class Metrics/{class_names[i]}/Train Recall vs Test Recall\",\n",
        "                                       {\"Train Recall\" : train_class_results[i][\"recall\"],\n",
        "                                        \"Test Recall\" : test_class_results[i][\"recall\"]},\n",
        "                                       (epoch + 1) if not mbatch_group > 0\n",
        "                                             else (epoch + 1) * mbatch_count)\n",
        "    summary_writer.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdd7da3d",
      "metadata": {},
      "source": [
        "### 3.4 Data Augmentation\n",
        "\n",
        "During our experiments, we observed that the performances is strongly positively correlated with the amount of images used for the training. We wanted to see if a simple data augmentation would improve the performances of the model. We doubled the amount of images in the dataset by flipping horizontaly the images. The dataset being a set of images of common objects taken in a diverse set of conditions, the right and left of the images are interchangeable (as opposed to the top and bottom  where for example the sky is on the top of the images). This horizontal flip does not add images that would be outside of the original distribution.\n",
        "\n",
        "Our experiments showed that this data augmentation does not improve the performances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf62c05",
      "metadata": {},
      "outputs": [],
      "source": [
        "from os import listdir, path, makedirs, remove\n",
        "from PIL import ImageOps,Image\n",
        "from shutil import copy\n",
        "\n",
        "def generate_mirrored_images():\n",
        "\n",
        "\timg_dir_train = path.join(\"ms-coco\",\"images\",\"train-resized\")\n",
        "\tmirror_img_dir_train = path.join(\"ms-coco\",\"images\",\"mir-train-resized\")\n",
        "\tlabel_dir_train = path.join(\"ms-coco\",\"labels\",\"train\")\n",
        "\tmirror_label_dir_train = path.join(\"ms-coco\",\"labels\",\"mir-train\")\n",
        "\t\n",
        "\n",
        "\n",
        "\tif not path.exists(mirror_img_dir_train) or len(listdir(mirror_img_dir_train)) < len(listdir(img_dir_train)):\n",
        "\t\tmakedirs(mirror_img_dir_train,exist_ok=True)\n",
        "\t\tfor im_name in listdir(img_dir_train):\n",
        "\t\t\twith Image.open(path.join(img_dir_train,im_name)) as img:\n",
        "\t\t\t\tImageOps.mirror(img).save(path.join(mirror_img_dir_train,\"mir\" + im_name))\n",
        "\tprint(len(listdir(mirror_img_dir_train)))\n",
        "\n",
        "\t\t\n",
        "\tif not path.exists(mirror_label_dir_train) or len(listdir(mirror_label_dir_train)) < len(listdir(label_dir_train)):\n",
        "\t\tmakedirs(mirror_label_dir_train,exist_ok=True)\n",
        "\t\tfor cls_name in listdir(label_dir_train):\n",
        "\t\t\tcopy(path.join(label_dir_train,cls_name),path.join(mirror_label_dir_train,\"mir\" + cls_name))\n",
        "\n",
        "\tprint(len(listdir(mirror_label_dir_train)))\n",
        "\treturn {\"images\":mirror_img_dir_train,\"labels\" : mirror_label_dir_train, \"prefix\" : \"mir\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "138e0619",
      "metadata": {},
      "source": [
        "## 4 Dataset analysis\n",
        "### 4.1 images\n",
        "In this cell, we display random images from the dataset alongside their lables to get a sense of the nature of the dataset and test the effects of the preprocessing transformation we apply. We quickly noticed that most of images from the dataset are not square with each images being at most 224 pixels on the width and/or the height and the other side being smaller.\n",
        "This is a problem because  the images need to be square to fit into the pretrained model. The transformation which comes with the pretrained model automatically adjusts the images by croping them. This works for adapting the images but it can crop an object near the borders of the images and therfore making the identification of certain labels imposible. This is why we created the `SquarePad` transform class to pad the images to feed only $224 \\times 224$ images protecting the images from the cropping.\n",
        "\n",
        "Even if it seemed like a good idea, our experiments show that the model loses performances withe the padding even with the reflection pading. This could be explained by the fact that the region of the padding is disrupting the pretraind model which was not trained on images with such padding. The problem of cropped object may be leess prevalent than we thought because moste images have the objects in the center."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e00eb262",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import RegNet_Y_1_6GF_Weights\n",
        "\n",
        "from os import listdir, path\n",
        "minibatch_size = 2\n",
        "\n",
        "img_dir_train = path.join(\"ms-coco\",\"images\",\"train-resized\")\n",
        "label_dir_train = path.join(\"ms-coco\",\"labels\",\"train\")\n",
        "\n",
        "n_images_train = len(listdir(img_dir_train))\n",
        "class SquarePad(torch.nn.Module):\n",
        "    def forward(self, img):\n",
        "        w,h = img.size\n",
        "        pad = ((224-w)//2,(224-h)//2)\n",
        "        return transforms.functional.pad(img,pad)\n",
        "\n",
        "transform = transforms.Compose([SquarePad(),\n",
        "                                transforms.ToTensor(),\n",
        "                                RegNet_Y_1_6GF_Weights.DEFAULT.transforms()])\n",
        "\n",
        "train_set = COCOTrainImageDataset(img_dir = img_dir_train,\n",
        "                          annotations_dir=label_dir_train,transform = transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=minibatch_size,\n",
        "                                           shuffle=True, num_workers=0)\n",
        "\n",
        "\n",
        "def imshow(ax, img, title):\n",
        "    img = img / 2 + 0.5 # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    ax.imshow(np.transpose(npimg, (1, 2, 0))) # NumPy expects (W, H, C)\n",
        "    ax.set_title(title)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "it = iter(train_loader) # create an iterator\n",
        "images, labels = next(it) # get the next mini-batch\n",
        "\n",
        "fig, ax = plt.subplots( minibatch_size,1) # create a 1 x 4 sub-plot\n",
        "\n",
        "for i, (img, label) in enumerate(zip(images, labels)):\n",
        "\n",
        "    imshow(ax[i], img, str([classes[l] for l in label.nonzero().squeeze(1)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f0b4729",
      "metadata": {},
      "source": [
        "### Labels\n",
        "We analysed the distribution of labels throughout the dataset. As we can see in the plot below, the distribution of the classes is very unequal. This is why we used a weighted loss in the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69ecff1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "try :\n",
        "\tclasses_stats = dict(np.load(\"label_stats.npy\",allow_pickle=True).item().most_common())\n",
        "except:\n",
        "\tlist_classes = []\n",
        "\tlist_label_files =listdir(label_dir_train)\n",
        "\tfor labels_file_name in list_label_files:\n",
        "\t\twith open(path.join(label_dir_train,labels_file_name),'r') as f:\n",
        "\t\t\tlist_classes.extend([int(label) for label in f.readlines()])\n",
        "\t\n",
        "\tclasses_stats = dict(Counter(list_classes).most_common())\n",
        "\tnp.save(\"label_stats.npy\",classes_stats) \n",
        "\n",
        "classes_name = []\n",
        "occurence = []\n",
        "for k,v in classes_stats.items():\n",
        "    classes_name.append(classes[k])\n",
        "    occurence.append(v)\n",
        "plt.rcParams[\"figure.figsize\"] = (20,3)\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel(\"# occurence \")\n",
        "plt.bar(classes_name,occurence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7af63e9",
      "metadata": {},
      "source": [
        "## 5. training an validation of the model\n",
        "### The choice of the model\n",
        "We chose for this assignment the RegNet model architecture for our pretrained classification model. The architecture is described in the folowing article :\n",
        "\n",
        "Xu, J., Pan, Y., Pan, X., Hoi, S., Yi, Z., & Xu, Z. (2021, January 3). REGNET: Self-Regulated Network for Image Classification. arXiv.org. https://arxiv.org/abs/2101.00590\n",
        "\n",
        "Th main idea of the architecture is to cobine a ResNet with a recurrent neural network adapted for image proceesing by replacing fully-connected layers by convolutional layers. At each step\n",
        "the input $X^t_1$ is proccessed by a convolutional layer which outputs $X^t_2$ and passed through a  convolutional recurrent neural network(convRNN) which also takes $H^{t-1}$ as input and outputs  hidden states $H^t$ which are concatenated with $X^t_2$ and processed by two more convolutional layers  and finaly added to the skip connection (see fig.3 from the article). The convRNN block allows to remember important infomations from the previous layers aloowing a more long term and selective memory than a simple skip connection.\n",
        "\n",
        "For the specific version of the pretrained model we chose the regnet_y_1_6gf from the [pytorch website](https://docs.pytorch.org/vision/stable/models.html) beacause it seeme to be performant while remaining relatively lightweight with 11.2 million parameters.\n",
        "\n",
        "### Experimentations\n",
        "For our experiments we explored different hyper-parameters and configurations with a subset of the dataset to quickly test ideas before validating these configurations with longer trainings. We did not encouter an over-fitting during our tests, this is why we do not use regularisation techniques. We used the tensorBoard to monitor the training.\n",
        "\n",
        "With our experiments, we concluded tha that the best hyper-parameters are :\n",
        "-  batch_size = 32\n",
        "-  learning_rate = 0.001\n",
        "-  freeze all but last layer\n",
        "-  no data augmentation\n",
        "-  no weight decay\n",
        "-  weighted loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "451dda87",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import statements for python, torch and companion libraries and your own modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import RegNet_Y_1_6GF_Weights, regnet_y_1_6gf\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "from os import listdir, path\n",
        "%load_ext tensorboard\n",
        "# global variables defining training hyper-parameters among other things \n",
        "epochs = 20\n",
        "num_workers = 0 # crashes if > 0\n",
        "minibatch_size = 32\n",
        "learning_rate = 0.001\n",
        "threshold = 0.5\n",
        "data_augmentation = None # generate_mirrored_images() the generation takes around 15 minutes\n",
        "\n",
        "# device initialization\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "\n",
        "# data directories initialization\n",
        "img_dir_train = path.join(\"ms-coco\",\"images\",\"train-resized\")\n",
        "label_dir_train = path.join(\"ms-coco\",\"labels\",\"train\")\n",
        "\n",
        "img_dir_test = path.join(\"ms-coco\",\"images\",\"test-resized\")\n",
        "\n",
        "\n",
        "# instantiation of transforms, datasets and data loaders\n",
        "# TIP : use torch.utils.data.random_split to split the training set into train and validation subsets\n",
        "n_images_train = len(listdir(img_dir_train)) if data_augmentation is None else 2*len(listdir(img_dir_train))\n",
        "n_images_test = len(listdir(img_dir_test))\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                RegNet_Y_1_6GF_Weights.DEFAULT.transforms()])\n",
        "\n",
        "# mini set to test code\n",
        "\n",
        "\"\"\" full_train_dataset = COCOTrainImageDataset(\n",
        "    img_dir=img_dir_train,\n",
        "    annotations_dir=label_dir_train,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_subset = torch.utils.data.Subset(full_train_dataset, range(0, min(2000, len(full_train_dataset))))\n",
        "val_subset = torch.utils.data.Subset(full_train_dataset, range(min(2000, len(full_train_dataset)), min(4000, len(full_train_dataset))))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=minibatch_size,\n",
        "                            shuffle=True, num_workers=num_workers)\n",
        "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=minibatch_size,\n",
        "                            shuffle=False, num_workers=num_workers)\n",
        "\n",
        "print(\"Train samples:\", len(train_subset), \"Validation samples:\", len(val_subset)) \"\"\" \n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(\n",
        "    COCOTrainImageDataset(img_dir = img_dir_train,\n",
        "                          annotations_dir=label_dir_train,\n",
        "                          transform=transform,\n",
        "                          data_augmentation_paths=data_augmentation),\n",
        "                          lengths=[n_images_train - n_images_test, n_images_test])\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=minibatch_size,\n",
        "                                           shuffle=True, num_workers=num_workers)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=minibatch_size,\n",
        "                                         shuffle=False, num_workers=num_workers)\n",
        "\n",
        "print(n_images_train,len(train_set),len(val_set))\n",
        "\n",
        "\n",
        "# instantiation and preparation of network model\n",
        "model = regnet_y_1_6gf(weights=RegNet_Y_1_6GF_Weights.DEFAULT)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "model.fc = nn.Linear(888, 80)\n",
        "\n",
        "#print(summary(model))\n",
        "model = model.to(device)\n",
        "\n",
        "# instantiation of loss criterion with weights\n",
        "classes_weight = torch.tensor([sum(classes_stats.values())/(len(classes)*classes_stats[i]) for i in range(len(classes))]).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(weight=classes_weight)\n",
        "#pondérer selon les classes sous représentés\n",
        "\n",
        "# instantiation of optimizer, registration of network parameters\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
        "\n",
        "# definition of current best model path\n",
        "best_model_path = \"./best_reg.pth\"\n",
        "best_f1_path = \"./best_f1.npy\"\n",
        "\n",
        "\n",
        "# initialization of model selection metric\n",
        "try :\n",
        "\tbest_val_f1 = np.load(best_f1_path).item()\n",
        "except :\n",
        "    best_val_f1 = 0.0\n",
        "    \n",
        "print(f\"Best F1 : {best_val_f1}\")\n",
        "\n",
        "# creation of tensorboard SummaryWriter (optional)\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# epochs loop:\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch [{epoch+1}/{epochs}]\")\n",
        "    \n",
        "    #   train\n",
        "    train_loss = train_loop(train_loader, model, criterion, optimizer, device)\n",
        "    train_results = {\"loss\": train_loss, \"accuracy\": 0, \"f1\": 0,\"precision\": 0, \"recall\": 0}\n",
        "    print(f\"Train   -> Loss: {train_results['loss']:.4f}\")\n",
        "\n",
        "    \n",
        "    #   validate on validation set\n",
        "    val_results = validation_loop(val_loader, model, criterion,\n",
        "                                  num_classes=len(classes),\n",
        "                                  device=device,\n",
        "                                  multi_task=True,\n",
        "                                  th_multi_task=threshold,\n",
        "                                  one_hot=True)\n",
        "    print(f\"Val   -> Loss: {val_results['loss']:.4f} F1: {val_results['f1']:.4f}\")\n",
        "    \n",
        "    #   update graphs (optional)\n",
        "    update_graphs(writer,epoch,train_results,val_results)\n",
        "    \n",
        "    #   is new model better than current model ?\n",
        "    #       save it, update current best metric\n",
        "    if val_results[\"f1\"] > best_val_f1:\n",
        "        best_val_f1 = val_results[\"f1\"]\n",
        "        np.save(\"best_f1.npy\",best_val_f1)\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"New best model saved with F1={best_val_f1:.4f}\")\n",
        "\n",
        "\n",
        "    # close tensorboard SummaryWriter if created (optional)\n",
        "    writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41cd8fc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = regnet_y_1_6gf(weights=RegNet_Y_1_6GF_Weights.DEFAULT)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "model.fc = nn.Linear(888, 80)\n",
        "model.load_state_dict(torch.load(\"best_reg.pth\", map_location=device))\n",
        "model = model.to(device)\n",
        "val_results,class_results = validation_loop(val_loader, model, criterion,\n",
        "                                  num_classes=len(classes),\n",
        "                                  device=device,\n",
        "                                  multi_task=True,\n",
        "                                  th_multi_task=threshold,\n",
        "                                  one_hot=True,\n",
        "                                  class_metrics=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc390d89",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Val   -> Loss: {val_results['loss']:.4f} F1: {val_results['f1']:.4f}\")\n",
        "spaces = 15\n",
        "combined = dict(zip(classes,[f1['f1'] for f1 in class_results]))\n",
        "sorted_classes = dict(sorted(combined.items(), key=lambda item: item[1]))\n",
        "total_labels = sum(classes_stats.values())\n",
        "\n",
        "print(f\"{(spaces+3)*\" \"}F1       proportion\")\n",
        "for label,f1 in sorted_classes.items():\n",
        "    print(f\"{label:} : {(spaces-len(label))*\" \"}{f1:.4f}  {classes_stats[classes.index(label)]/total_labels * 100: .3f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33d6599c",
      "metadata": {},
      "outputs": [],
      "source": [
        "occurences = []\n",
        "F1s = []\n",
        "for label,f1 in sorted_classes.items():\n",
        "    occurences.append(classes_stats[classes.index(label)])\n",
        "    F1s.append(f1)\n",
        "plt.xlabel(\"# occurences\")\n",
        "plt.ylabel(\"F1 score for each class\")\n",
        "plt.scatter(occurences,F1s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c8f9183",
      "metadata": {},
      "source": [
        "We can see from the previous cell that the  most common classes tend to be those with the best F1 score even if there is a large group of class that over performs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae4855a7",
      "metadata": {},
      "source": [
        "## 6. test submission program\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d3c713a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import statements for python, torch and companion libraries and your own modules\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import RegNet_Y_1_6GF_Weights, regnet_y_1_6gf\n",
        "\n",
        "from os import listdir, path\n",
        "\n",
        "\n",
        "\n",
        "# global variables defining inference hyper-parameters among other things \n",
        "minibatch_size = 32\n",
        "threshold = 0.5\n",
        "num_workers = 0\n",
        "\n",
        "# data, trained model and output directories/filenames initialization\n",
        "img_dir_test = path.join(\"ms-coco\",\"images\",\"test-resized\")\n",
        "output_path = \"./submission.json\"\n",
        "\n",
        "\n",
        "# device initialization\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# instantiation of transforms, dataset and data loader\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                RegNet_Y_1_6GF_Weights.DEFAULT.transforms()])\n",
        "test_set = COCOTestImageDataset(img_dir = img_dir_test, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=minibatch_size,\n",
        "                            shuffle=False, num_workers=num_workers)\n",
        "\n",
        "# load network model from saved file\n",
        "model = regnet_y_1_6gf(weights=RegNet_Y_1_6GF_Weights.DEFAULT)\n",
        "\n",
        "model.fc = nn.Linear(888, 80)\n",
        "model.load_state_dict(torch.load(\"best_reg.pth\", map_location=device))\n",
        "model = model.to(device)\n",
        "\n",
        "# initialize output dictionary\n",
        "output_dict = {}\n",
        "\n",
        "# prediction loop over test_loader\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        #get mini-batch\n",
        "        images, ids = data[0].to(device), data[1]\n",
        "        #compute network output\n",
        "        outputs = model(images)\n",
        "        #threshold network output\n",
        "        predictions = torch.where(outputs > threshold, 1.0, 0.0).cpu()\n",
        "\n",
        "        #update dictionary entries write corresponding class indices\n",
        "        for id, pred in zip(ids,predictions):\n",
        "            output_dict[id] = pred.nonzero().squeeze(1).tolist()\n",
        "        \n",
        "\n",
        "# write JSON file\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(output_dict, f, ensure_ascii=False, indent=4)\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
